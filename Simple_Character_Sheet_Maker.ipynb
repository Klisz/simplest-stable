{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to images/.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec3747b2e584e79a2e28e0bd3023cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the seed 1725630141\n",
      "Generation of images failed for some reason, try loading another model. If that doesn't work, go to Runtime -> Disconnect and Delete Runtime and then rerun this cell, and then if that fails, take a screenshot or copy the error below and ask @Cadaeic about it!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 221\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGeneration of images failed for some reason, try loading another model. If that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt work, go to Runtime -> Disconnect and Delete Runtime and then rerun this cell, and then if that fails, take a screenshot or copy the error below and ask @Cadaeic about it!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m   \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[1], line 212\u001b[0m\n\u001b[0;32m    209\u001b[0m opt[\u001b[39m\"\u001b[39m\u001b[39mcontrolnet_model\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m controlnet\n\u001b[0;32m    211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m   pipe, images, images_details \u001b[39m=\u001b[39m process_and_generate(\n\u001b[0;32m    213\u001b[0m     opt \u001b[39m=\u001b[39;49m opt,\n\u001b[0;32m    214\u001b[0m     pipe \u001b[39m=\u001b[39;49m pipe, \n\u001b[0;32m    215\u001b[0m     progress\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    216\u001b[0m     randomizer \u001b[39m=\u001b[39;49m randomizers,\n\u001b[0;32m    217\u001b[0m     display_and_print\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGeneration of images failed for some reason, try loading another model. If that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt work, go to Runtime -> Disconnect and Delete Runtime and then rerun this cell, and then if that fails, take a screenshot or copy the error below and ask @Cadaeic about it!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\src\\process.py:146\u001b[0m, in \u001b[0;36mprocess_and_generate\u001b[1;34m(opt, pipe, progress, randomizer, display_and_print)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     prompt_options[\u001b[39m\"\u001b[39m\u001b[39mnegative_prompt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m process_prompt_and_add_keyword(\n\u001b[0;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, opt[\u001b[39m\"\u001b[39m\u001b[39mnegative_keyword\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m opt[\u001b[39m\"\u001b[39m\u001b[39madd_keyword\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, randomizer)\n\u001b[1;32m--> 146\u001b[0m image \u001b[39m=\u001b[39m pipe(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_options)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[0;32m    147\u001b[0m image_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbatch_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m settings_info \u001b[39m=\u001b[39m save_image(\n\u001b[0;32m    150\u001b[0m     image, image_name, prompt_options, opt, seed, opt[\u001b[39m\"\u001b[39m\u001b[39moutputs_folder\u001b[39m\u001b[39m\"\u001b[39m], opt[\u001b[39m\"\u001b[39m\u001b[39mprogram_version\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\src\\SimpleStableDiffusionPipeline.py:1196\u001b[0m, in \u001b[0;36mSimpleStableDiffusionPipeline.__call__\u001b[1;34m(self, prompt, negative_prompt, image, mask_image, height, width, num_inference_steps, guidance_scale, strength, num_images_per_prompt, eta, generator, latents, max_embeddings_multiples, output_type, return_dict, callback, is_cancelled_callback, callback_steps, controlnet_model, controlnet_image, cross_attention_kwargs, controlnet_conditioning_scale, latent_noise_inpaint, **kwargs)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[39m# 6. Prepare latent variables\u001b[39;00m\n\u001b[1;32m-> 1196\u001b[0m latents, init_latents_orig, noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_latents(\n\u001b[0;32m   1197\u001b[0m     image\u001b[39m=\u001b[39;49mimage,\n\u001b[0;32m   1198\u001b[0m     timestep\u001b[39m=\u001b[39;49mlatent_timestep,\n\u001b[0;32m   1199\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size \u001b[39m*\u001b[39;49m num_images_per_prompt,\n\u001b[0;32m   1200\u001b[0m     height\u001b[39m=\u001b[39;49mheight,\n\u001b[0;32m   1201\u001b[0m     width\u001b[39m=\u001b[39;49mwidth,\n\u001b[0;32m   1202\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1203\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m   1204\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[0;32m   1205\u001b[0m     latents\u001b[39m=\u001b[39;49mlatents,\n\u001b[0;32m   1206\u001b[0m     mask\u001b[39m=\u001b[39;49mmask \u001b[39mif\u001b[39;49;00m latent_noise_inpaint \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1207\u001b[0m )\n\u001b[0;32m   1209\u001b[0m \u001b[39m# 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m extra_step_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_extra_step_kwargs(generator, eta)\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\src\\SimpleStableDiffusionPipeline.py:1000\u001b[0m, in \u001b[0;36mSimpleStableDiffusionPipeline.prepare_latents\u001b[1;34m(self, image, timestep, batch_size, height, width, dtype, device, generator, latents, mask)\u001b[0m\n\u001b[0;32m    998\u001b[0m     \u001b[39mreturn\u001b[39;00m latents, init_latents_orig, noise\n\u001b[0;32m    999\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# experimentally completely replacing the masked area with latent noise\u001b[39;00m\n\u001b[1;32m-> 1000\u001b[0m     init_latent_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvae\u001b[39m.\u001b[39;49mencode(image)\u001b[39m.\u001b[39mlatent_dist\n\u001b[0;32m   1001\u001b[0m     init_latents \u001b[39m=\u001b[39m init_latent_dist\u001b[39m.\u001b[39msample(generator\u001b[39m=\u001b[39mgenerator)\n\u001b[0;32m   1002\u001b[0m     init_latents \u001b[39m=\u001b[39m \u001b[39m0.18215\u001b[39m \u001b[39m*\u001b[39m init_latents\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\diffusers\\utils\\accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_hf_hook\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook, \u001b[39m\"\u001b[39m\u001b[39mpre_forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(\u001b[39mself\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\diffusers\\models\\autoencoder_kl.py:158\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[1;34m(self, x, return_dict)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_tiling \u001b[39mand\u001b[39;00m (x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtile_sample_min_size \u001b[39mor\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtile_sample_min_size):\n\u001b[0;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiled_encode(x, return_dict\u001b[39m=\u001b[39mreturn_dict)\n\u001b[1;32m--> 158\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    159\u001b[0m moments \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_conv(h)\n\u001b[0;32m    160\u001b[0m posterior \u001b[39m=\u001b[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\diffusers\\models\\vae.py:101\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    100\u001b[0m     sample \u001b[39m=\u001b[39m x\n\u001b[1;32m--> 101\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_in(sample)\n\u001b[0;32m    103\u001b[0m     \u001b[39m# down\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[39mfor\u001b[39;00m down_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_blocks:\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mb:\\Programming\\Simple_Stable_With_Diffusers\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "model = \"Dreamlike Diffusion\" #@param [\"Stable Diffusion 1.5\", 'Dreamlike Diffusion', 'Openjourney', 'Openjourney V2 Beta', 'seek.art_MEGA', 'Sygil Diffusion', 'Vintedois Diffusion', 'Analog Diffusion', 'Dreamlike Photoreal', 'Anything v3.0', 'Waifu Diffusion 1.3', 'Pastel Waifu Diffusion', 'OpenNiji', 'Untitled', 'Inkpunk Diffusion', 'Vivid Watercolors', 'Van Gogh Diffusion', 'Modern Art Style', 'Corporate Memphis', 'Pokemon Diffusion', 'Robo Diffusion', 'Classic Animation Diffusion', 'Modern Animation Diffusion', 'Ghibli Diffusion', 'Furry Diffusion'] {type:\"string\"}\n",
    "model = \"Anything v3.0\"\n",
    "\n",
    "prompt = \"male focus, 1boy, solo, long coat, black coat, short blue hair, character sheet, character turnaround\" #@param {type:\"string\"}\n",
    "negative_prompt = \"1girl, woman, distorted, mutation, dull, kitsch\" #@param {type:\"string\"}\n",
    "number_of_images = 1 #@param {type:\"integer\"}\n",
    "steps = 20 #@param {type:\"integer\"}\n",
    "sampler = \"UniPC\" #@param [\"Euler a\", \"Euler\", \"KLMS\", \"DPMSolver++ (2S) (has issues with img2img)\", \"DPMSolver++ (2M)\", \"UniPC\"] \n",
    "guidance_scale = 7 #@param {type:\"slider\", min:0, max:20, step:0.5}\n",
    "seed = -1 #@param {type:\"integer\"}   \n",
    "upscale_results = True #@param {type: \"boolean\"}\n",
    "upscale_strength = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "\n",
    "add_charturner_embedding = False #@param {type: \"boolean\"}\n",
    "add_standard_danbooru_model_quality_prompts = True #@param {type: \"boolean\"}\n",
    "add_automatic_keyword = True #@param {type: \"boolean\"}\n",
    "\n",
    "######\n",
    "\n",
    "test_mode = True\n",
    "enable_attention_slicing = False\n",
    "enable_xformers = False\n",
    "enable_gpu_offload = False #does not work yet\n",
    "should_load_downloaded_embeds = False\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "try:\n",
    "  import torch\n",
    "  %cd /content/simplest-stable/\n",
    "  import src\n",
    "except ImportError as e:\n",
    "  import os, subprocess\n",
    "  print(\"Above error is intentional, don't worry\")\n",
    "  print(\"Installing libraries...\")\n",
    "  %cd /content/\n",
    "  subprocess.run([\"git\", \"clone\", \"https://github.com/cadaeix/simplest-stable.git\"])\n",
    "  %cd /content/simplest-stable/\n",
    "  if test_mode:\n",
    "    subprocess.run([\"git\", \"checkout\", \"test-branch\"])\n",
    "  if enable_xformers:\n",
    "    subprocess.run(['pip', 'install', 'triton==2.0.0.dev20221202', 'xformers==0.0.16rc424'])\n",
    "  if enable_gpu_offload:\n",
    "    subprocess.run(['pip', 'install', 'git+https://github.com/huggingface/accelerate.git'])\n",
    "  subprocess.run([\"pip3\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "  downloaded_embeds_path = None\n",
    "\n",
    "# Pillow occasionally has odd import errors\n",
    "from src.import_patch import patch_import_errors\n",
    "patch_import_errors()\n",
    "\n",
    "import os, subprocess, sys, json, logging\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from diffusers import ControlNetModel\n",
    "from src.utils import free_ram, find_custom_models\n",
    "from src.loading import prepare_pipe, get_model_file_from_civitai_with_model_id, load_embeddings\n",
    "from src.process import process_and_generate\n",
    "from src.randomizer import get_default_random_lists_from_folder, get_random_lists_from_folder\n",
    "\n",
    "logging.disable(sys.maxsize)\n",
    "clear_output(wait=False)\n",
    "\n",
    "downloaded_embeds_path = \"embeds/\"\n",
    "if not os.path.exists(downloaded_embeds_path):\n",
    "  subprocess.run([\"mkdir\", \"-p\", downloaded_embeds_path])\n",
    "\n",
    "try:\n",
    "  outputs_folder\n",
    "  if not os.path.exists(outputs_folder):\n",
    "      os.mkdirs(outputs_folder)\n",
    "  session_folder = outputs_folder\n",
    "except NameError as e:\n",
    "  outputs_path = \"images/\"\n",
    "  if not os.path.exists(outputs_path):\n",
    "      os.mkdir(outputs_path)\n",
    "      \n",
    "  print(f\"Outputs will be saved to {outputs_path}.\")\n",
    "  session_folder = os.path.join(outputs_path, datetime.now().strftime(\"%Y_%m_%d\"))\n",
    "  if not os.path.exists(session_folder):\n",
    "      os.mkdir(session_folder)\n",
    "\n",
    "try:\n",
    "  model_dict\n",
    "except NameError as e:\n",
    "  with open('src/resources/models.json') as modelfile:\n",
    "      model_dict = json.load(modelfile)\n",
    "  del modelfile\n",
    "\n",
    "default_randomizers = get_default_random_lists_from_folder(\n",
    "    \"src/resources/randomizers\")\n",
    "try: \n",
    "  custom_randomizer_folder\n",
    "  if os.path.exists(custom_randomizer_folder):\n",
    "    randomizers = get_random_lists_from_folder(custom_randomizer_folder)\n",
    "    randomizers = {**default_randomizers, **randomizers}\n",
    "  else:\n",
    "    randomizers = default_randomizers\n",
    "except NameError as e:\n",
    "  randomizers = default_randomizers\n",
    "\n",
    "model_type_mode = \"Downloadable Models\"\n",
    "try:\n",
    "  custom_models_folder\n",
    "  custom_models_path = custom_models_folder if os.path.exists(custom_models_folder) else None\n",
    "  custom_model\n",
    "  use_custom_model\n",
    "  if use_custom_model and os.path.exists(custom_model):\n",
    "    model_type_mode = \"Custom Models\"\n",
    "    model = custom_model\n",
    "    custom_model_dict, _ = find_custom_models(custom_models_path)\n",
    "except NameError as e:\n",
    "  custom_models_path = None\n",
    "  model_type_mode = \"Downloadable Models\"\n",
    "\n",
    "if add_charturner_embedding:\n",
    "  charturner_embedding_path = os.path.join(downloaded_embeds_path, \"charturner.pt\")\n",
    "  if not os.path.exists(charturner_embedding_path):\n",
    "    get_model_file_from_civitai_with_model_id(\"8387\", charturner_embedding_path)\n",
    "  should_load_downloaded_embeds = True\n",
    "    \n",
    "\n",
    "pose_sheet = Image.open(\"src/resources/char_sheet_pose.png\").convert('RGB')\n",
    "width = 768\n",
    "height = 512\n",
    "init_image = Image.open(\"src/resources/char_sheet_base.png\").convert('RGB')\n",
    "mask = Image.open(\"src/resources/char_sheet_mask.png\").convert('RGB')\n",
    "\n",
    "if add_standard_danbooru_model_quality_prompts:\n",
    "    prompt = \"masterpiece, best quality, \" + prompt\n",
    "    standard_negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username\"\n",
    "    negative_prompt = standard_negative if negative_prompt == None or negative_prompt == \"\" else standard_negative + \", \" + negative_prompt\n",
    "\n",
    "if add_charturner_embedding:\n",
    "  if \"charturner\" not in prompt:\n",
    "    prompt = \"(charturner:1), \" + prompt\n",
    "\n",
    "opt = {\n",
    "    \"model_name\" : model,\n",
    "    \"eta\" : 0.0,\n",
    "    \"steps\" : steps,\n",
    "    \"controlnet_image\": pose_sheet,\n",
    "    \"init_img\" : init_image,\n",
    "    \"mask_image\": mask,\n",
    "    \"latent_noise_inpaint\": True,\n",
    "    \"number_of_images\": number_of_images,\n",
    "    \"prompt\" : prompt,\n",
    "    \"negative\" : negative_prompt,\n",
    "    \"sampler\" : sampler,\n",
    "    \"scale\" : guidance_scale,\n",
    "    \"seed\" : seed,\n",
    "    \"strength\" : 1,\n",
    "    \"H\" : 512,\n",
    "    \"W\" : 768,\n",
    "    #upscale stuff\n",
    "    \"passes\" : 1,\n",
    "    \"upscale\": upscale_results,\n",
    "    \"upscale_strength\" : upscale_strength,\n",
    "    \"detail_scale\" : 10,\n",
    "    \"add_keyword\": add_automatic_keyword,\n",
    "    \"prediction_type\": model_dict[model][\"prediction\"],\n",
    "    \"keyword\": model_dict[model].get(\"keyword\"),\n",
    "    \"negative_keyword\": model_dict[model].get(\"negative_keyword\"),\n",
    "    \"outputs_folder\": session_folder,\n",
    "    \"program_version\": \"Simple Character Sheet Maker (Notebook, pre-release 20230306)\"\n",
    "    }\n",
    "\n",
    "recreate = False\n",
    "try:\n",
    "  controlnet\n",
    "  pipe\n",
    "  recreate = model_name != model\n",
    "  if recreate:\n",
    "    del pipe\n",
    "    free_ram()\n",
    "except NameError as e:\n",
    "  pipe = None\n",
    "  controlnet = None\n",
    "  recreate = True\n",
    "\n",
    "if recreate:\n",
    "  try:\n",
    "    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\n",
    "    pipe, pipe_info = prepare_pipe(\n",
    "      model_name = model, \n",
    "      model_type = model_type_mode, \n",
    "      downloadable_model_dict = model_dict,\n",
    "      custom_model_dict = None,\n",
    "      cached_model_dict = None,\n",
    "      enable_attention_slicing = enable_attention_slicing,\n",
    "      enable_xformers = enable_xformers,\n",
    "      to_cuda = False)\n",
    "    if should_load_downloaded_embeds:\n",
    "      pipe = load_embeddings(downloaded_embeds_path, pipe)\n",
    "    if enable_gpu_offload:\n",
    "        # this doesn't work yet\n",
    "        from accelerate import cpu_offload_with_hook\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        cpu_offload_with_hook(controlnet, torch.device(f\"cuda:0\"))\n",
    "    else:\n",
    "        controlnet.to(\"cuda\")\n",
    "        pipe.to(\"cuda\")\n",
    "    model_name = opt[\"model_name\"]\n",
    "  except Exception as e:\n",
    "    print(f\"Failed to load selected model for some reason, please try loading another model. If that doesn't work, go to Runtime -> Disconnect and Delete Runtime and then rerun this cell, and then if that fails, take a screenshot or copy the error below and ask @Cadaeic about it!\")\n",
    "    raise e\n",
    "\n",
    "opt[\"controlnet_model\"] = controlnet\n",
    "\n",
    "try:\n",
    "  pipe, images, images_details = process_and_generate(\n",
    "    opt = opt,\n",
    "    pipe = pipe, \n",
    "    progress=None,\n",
    "    randomizer = randomizers,\n",
    "    display_and_print=True\n",
    "    )\n",
    "except Exception as e:\n",
    "  print(f\"Generation of images failed for some reason, try loading another model. If that doesn't work, go to Runtime -> Disconnect and Delete Runtime and then rerun this cell, and then if that fails, take a screenshot or copy the error below and ask @Cadaeic about it!\")\n",
    "  raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
